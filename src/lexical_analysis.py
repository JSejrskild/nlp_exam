"""
Lexical Analysis

This script investigates the lexical features of the human and model-generated data 
based on multiple linguistic complexity/diversity metrics. 

The aim of this part of the analysis is to answer RQ1:
RQ1: Lexical features 
How lexically similar are responses generated by generic and fine-tuned LLMs compared to human therapist 
responses, based on quantitative measures of lexical richness and complexity?

What the script does: 
1. loads the merged dataset (final_df.csv)
2. computes per-row descriptive metrics using spaCy:
- word count
- Sentence count
- Average sentence length 
- Type-Token Ratio (TTR)
- Corrected Type-Token Ratio (CTTR)
- Measure of Textual Lexical Diversity (MTLD)
3. Produced both per-row analysis files and aggregated summary statistics. 
4. Saves a lexical summary file for reporting. 

Input: overview.csv 
Intermediate output: lexical_analysis.csv
Final output: lexical_summary.csv

"""

import spacy
import pandas as pd
import math
from tqdm import tqdm


# -------------------- Setup --------------------
nlp = spacy.load("en_core_web_sm")
tqdm.pandas()

# Paths
FINAL_DF_PATH = "../data_output/lexical_analysis/overview.csv"
LEXICAL_ANALYSIS_PATH = "../data_output/lexical_analysis/lexical_analysis.csv"
LEXICAL_SUMMARY_PATH = "../data_output/lexical_analysis/lexical_summary.csv"

# Read overview
df = pd.read_csv(FINAL_DF_PATH)

# Columns to analyze (human vs LLM)
TEXT_COLS = ["human_scene", "summary", "LLM_scene"]

# -------------------- Functions (unchanged) --------------------
def spacy_word_count(text):
    if not isinstance(text, str) or text.strip() == "":
        return 0
    doc = nlp(text)
    return sum(1 for t in doc if t.is_alpha)

def sentence_count(text):
    if not isinstance(text, str) or text.strip() == "":
        return 0
    return len(list(nlp(text).sents))

def avg_sentence_length(text):
    if not isinstance(text, str) or text.strip() == "":
        return 0
    sentences = list(nlp(text).sents)
    if len(sentences) == 0:
        return 0
    total_words = sum(len([t for t in sent if t.is_alpha]) for sent in sentences)
    return total_words / len(sentences)

def ttr_spacy(text):
    if not isinstance(text, str) or text.strip() == "":
        return 0
    tokens = [t.text.lower() for t in nlp(text) if t.is_alpha]
    if len(tokens) == 0: return 0
    return len(set(tokens))/len(tokens)

def cttr_spacy(text):
    if not isinstance(text, str) or text.strip() == "":
        return 0
    tokens = [t.text.lower() for t in nlp(text) if t.is_alpha]
    n_tokens = len(tokens)
    if n_tokens == 0: return 0
    return len(set(tokens)) / math.sqrt(2*n_tokens)

def mtld_calc(tokens, ttr_threshold=0.72, min_segment=10):
    if len(tokens)==0: return 0
    factors=0
    types=set()
    token_count=0
    for tok in tokens:
        token_count+=1
        types.add(tok)
        current_ttr=len(types)/token_count
        if token_count>=min_segment and current_ttr<=ttr_threshold:
            factors+=1
            types=set()
            token_count=0
    if token_count!=0:
        factors+=(1-(len(types)/token_count-ttr_threshold)/(1-ttr_threshold))
    return 0 if factors==0 else len(tokens)/factors

def mtld_spacy(text):
    if not isinstance(text, str) or text.strip() == "":
        return 0
    tokens = [t.text.lower() for t in nlp(text) if t.is_alpha]
    if len(tokens)==0: return 0
    return (mtld_calc(tokens)+mtld_calc(list(reversed(tokens))))/2

# -------------------- Main --------------------
if __name__=="__main__":
    # Initialize dataframe with filename for complete per-row analysis
    lex_df = pd.DataFrame()
    lex_df["filename"] = df["filename"]
    
    print("Computing word counts...")
    for col in TEXT_COLS:
        lex_df[f"{col}_word_count"] = df[col].progress_apply(spacy_word_count)
    
    print("Computing sentence metrics...")
    for col in TEXT_COLS:
        lex_df[f"{col}_sentence_count"] = df[col].progress_apply(sentence_count)
        lex_df[f"{col}_avg_sentence_length"] = df[col].progress_apply(avg_sentence_length)
    
    print("Computing TTR...")
    for col in TEXT_COLS:
        lex_df[f"{col}_TTR"] = df[col].progress_apply(ttr_spacy)
    
    print("Computing CTTR...")
    for col in TEXT_COLS:
        lex_df[f"{col}_CTTR"] = df[col].progress_apply(cttr_spacy)
    
    print("Computing MTLD...")
    for col in TEXT_COLS:
        lex_df[f"{col}_MTLD"] = df[col].progress_apply(mtld_spacy)
    
    # Save complete lexical analysis with ALL metrics per row (for logistic regression)
    lex_df.to_csv(LEXICAL_ANALYSIS_PATH, index=False)
    print(f"âœ“ Saved complete lexical analysis to {LEXICAL_ANALYSIS_PATH}")

    # Summary stats function (for plotting)
    def summary_stats(df_metrics):
        cols = [c for c in df_metrics.columns if c != "filename"]
        return pd.DataFrame({
            "Mean": df_metrics[cols].mean(),
            "Min": df_metrics[cols].min(),
            "Max": df_metrics[cols].max(),
            "SD": df_metrics[cols].std()
        }).round(3)

    # Create summary from the complete dataframe (for averaged plotting)
    lexical_summary = summary_stats(lex_df)
    lexical_summary.to_csv(LEXICAL_SUMMARY_PATH)
    print(f"âœ“ Saved lexical summary to {LEXICAL_SUMMARY_PATH}")
    
    print("\nâœ“ Lexical analysis finished")
    print(f"  - Per-row metrics (for regression): {LEXICAL_ANALYSIS_PATH}")
    print(f"  - Aggregated stats (for plotting): {LEXICAL_SUMMARY_PATH}")