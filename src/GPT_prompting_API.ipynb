{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab59f69",
   "metadata": {},
   "source": [
    "# API PROMPTING (OMG EXCITING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0404538",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.error'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merror\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RateLimitError  \u001b[38;5;66;03m# make sure this import works with your client\u001b[39;00m\n\u001b[32m     11\u001b[39m load_dotenv()  \u001b[38;5;66;03m# reads .env file automatically\u001b[39;00m\n\u001b[32m     13\u001b[39m client = OpenAI(api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai.error'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # reads .env file automatically\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a3a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65cc59ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5-search-api\n",
      "gpt-5-search-api-2025-10-14\n",
      "dall-e-2\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-audio\n",
      "gpt-4o-mini-tts\n",
      "gpt-4-turbo\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-4-0613\n",
      "gpt-4\n",
      "gpt-4.1\n",
      "gpt-4.1-2025-04-14\n",
      "text-embedding-ada-002\n",
      "dall-e-3\n",
      "gpt-4.1-nano\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "o1-2024-12-17\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-audio-2025-08-28\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "omni-moderation-latest\n",
      "o1-pro\n",
      "o1-pro-2025-03-19\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "text-embedding-3-small\n",
      "o1-mini-2024-09-12\n",
      "gpt-4o-2024-08-06\n",
      "o1\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-image-1-mini\n",
      "gpt-5-mini\n",
      "gpt-image-1\n",
      "gpt-5-chat-latest\n",
      "gpt-5-mini-2025-08-07\n",
      "omni-moderation-2024-09-26\n",
      "gpt-5\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-5-nano\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "tts-1-hd\n",
      "tts-1\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-audio-mini\n",
      "gpt-audio-mini-2025-10-06\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o3-mini-2025-01-31\n",
      "o3-mini\n",
      "o1-mini\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4-1106-preview\n",
      "babbage-002\n",
      "gpt-3.5-turbo\n",
      "gpt-5-2025-08-07\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-transcribe\n",
      "sora-2\n",
      "sora-2-pro\n",
      "gpt-5-pro-2025-10-06\n",
      "davinci-002\n",
      "gpt-4o\n",
      "gpt-4o-realtime-preview\n",
      "gpt-realtime-mini\n",
      "gpt-realtime-mini-2025-10-06\n",
      "o4-mini\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "o4-mini-2025-04-16\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "gpt-5-pro\n",
      "whisper-1\n",
      "gpt-4o-mini-transcribe\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "o3\n",
      "o3-2025-04-16\n",
      "gpt-4o-transcribe-diarize\n",
      "text-embedding-3-large\n",
      "gpt-5-codex\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-10-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6cdeb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n",
      "Rate limit hit. Waiting 3.00s before retry (attempt 1/5)...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Setting paths\n",
    "if \"__file__\" in globals():\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "else:\n",
    "    script_dir = os.getcwd()\n",
    "\n",
    "base_dir = os.path.abspath(os.path.join(script_dir, \"..\"))\n",
    "\n",
    "season1_path = os.path.join(base_dir, \"data_output\", \"season1_chunk.txt\")\n",
    "summaries_path = os.path.join(base_dir, \"data_output\", \"summaries_MEETING\", \"scene_summaries_MEETING.csv\")\n",
    "output_path = os.path.join(base_dir, \"data_output\", \"GPT5_scenes.csv\")\n",
    "\n",
    "# Feeding baseline info\n",
    "with open(season1_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    season1_text = f.read().strip()\n",
    "\n",
    "# GPT call with rate-limit handling\n",
    "def call_gpt_with_retry(prompt, max_retries=5):\n",
    "    delay = 3  # initial wait in seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-5-search-api\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=200\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:  # catch all exceptions\n",
    "            # Check if this is a rate limit error\n",
    "            if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                wait_time = delay\n",
    "                print(f\"Rate limit hit. Waiting {wait_time:.2f}s before retry (attempt {attempt+1}/{max_retries})...\")\n",
    "                time.sleep(wait_time)\n",
    "                delay *= 2  # exponential backoff\n",
    "            else:\n",
    "                raise e\n",
    "    raise Exception(\"Max retries reached due to rate limits.\")\n",
    "\n",
    "# Build the prompt\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"Ignore all previous messages.\n",
    "You have access to the following baseline text from Season 1:\n",
    "---\n",
    "{season1_text}\n",
    "---\n",
    "Now make a manuscript for a scene in Season 2.\n",
    "Ensure the scene sticks to the actions as explained by the summary.\n",
    "Keep the tone and characters true to the first season.\n",
    "Do your best to keep the number of lines similar to the number given.\n",
    "\n",
    "Summary: {row['summary']}\n",
    "Number of lines: {row['num_lines']}\n",
    "\"\"\"\n",
    "\n",
    "# Process CSV\n",
    "results = []\n",
    "with open(summaries_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        prompt = build_prompt(row)\n",
    "        response = call_gpt_with_retry(prompt)\n",
    "        results.append({\n",
    "            \"num_lines\": row[\"num_lines\"],\n",
    "            \"summary\": row[\"summary\"],\n",
    "            \"scene\": response\n",
    "        })\n",
    "        # Optional small sleep to reduce bursts\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Write output CSV\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"num_lines\", \"summary\", \"scene\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911603bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",     # INPUT THE MODEL TO BE USED HERE\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"Ignore all previous messages.\n",
    "Answer only this context and nothing else.\n",
    "\n",
    "ID: {row['ID']}\n",
    "Context: {row['Context']}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"context_ID.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    results = []\n",
    "    for row in reader:\n",
    "        prompt = build_prompt(row)\n",
    "        response = call_gpt(prompt)\n",
    "        results.append({\"ID\": row[\"ID\"], \"Context\": row[\"Context\"], \"response\": response})\n",
    "\n",
    "with open(\"GPT_responses.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"ID\", \"Context\", \"response\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
